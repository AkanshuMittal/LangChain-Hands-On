{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e35a617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f486e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a4cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(\"Who was the winner of the IPL 2016?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544144cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The winner of the Indian Premier League (IPL) 2016 was Sunrisers Hyderabad. They won the title by defeating Royal Challengers Bangalore in the final.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818e429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a63bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is GenAI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ada553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**GenAI** is short for **Generative Artificial Intelligence**.\\n\\nIn simple terms, it\\'s a type of artificial intelligence that can **create new content**, rather than just analyzing or processing existing data. This content can take many forms, including:\\n\\n*   **Text:** Writing articles, stories, poems, code, emails, scripts, and even entire conversations.\\n*   **Images:** Generating realistic or artistic images from text descriptions, or modifying existing images.\\n*   **Audio:** Creating music, voiceovers, sound effects, and even synthesizing human-like speech.\\n*   **Video:** Producing short video clips, animations, and even generating entire video sequences.\\n*   **Code:** Writing software code in various programming languages.\\n*   **3D Models:** Designing and generating three-dimensional objects.\\n\\n**How does it work?**\\n\\nGenAI models are typically trained on massive datasets of existing content. Through this training, they learn patterns, structures, styles, and relationships within the data. When you provide a prompt or input, the GenAI model uses its learned knowledge to generate new content that is similar in style and characteristics to the data it was trained on, but also original.\\n\\n**Key characteristics of GenAI:**\\n\\n*   **Generative:** Its primary function is to produce new output.\\n*   **Probabilistic:** It doesn\\'t have a single \"right\" answer. Instead, it generates outputs based on probabilities learned from its training data. This is why you might get slightly different results each time you ask it to generate something.\\n*   **Learns patterns:** It identifies underlying patterns, relationships, and nuances in the data it\\'s trained on.\\n*   **Versatile:** It can be applied to a wide range of creative and functional tasks.\\n\\n**Examples of GenAI in action:**\\n\\n*   **ChatGPT (OpenAI):** A popular text-based GenAI model that can answer questions, write essays, summarize information, and much more.\\n*   **DALL-E (OpenAI) and Midjourney:** Image generation models that create visuals from text prompts.\\n*   **Stable Diffusion:** Another powerful open-source image generation model.\\n*   **GitHub Copilot:** An AI pair programmer that suggests code snippets and entire functions.\\n*   **MusicLM (Google):** A model that can generate music from text descriptions.\\n\\n**Why is GenAI important?**\\n\\nGenAI is considered a significant advancement in AI because it moves beyond analysis and towards creation. It has the potential to:\\n\\n*   **Boost creativity:** Help artists, writers, musicians, and designers overcome creative blocks and explore new ideas.\\n*   **Automate tasks:** Streamline content creation processes for businesses, marketers, and developers.\\n*   **Personalize experiences:** Generate tailored content for individual users.\\n*   **Accelerate research and development:** Aid in hypothesis generation and data exploration.\\n*   **Democratize creation:** Make sophisticated content creation tools accessible to a wider audience.\\n\\nWhile GenAI is incredibly powerful and has many exciting applications, it\\'s also important to be aware of its limitations and potential ethical considerations, such as the generation of misinformation, copyright issues, and bias in the generated content.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7d8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[(\"system\", \"You are a GenAI Developer\"),\n",
    "         (\"user\", \"Tell me how to make a Q-NA Chatbot using RAG\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e65798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e8067a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Making a Q&A chatbot using Retrieval Augmented Generation (RAG) is a fantastic way to build a chatbot that can answer questions based on your specific documents or knowledge base. Here\\'s a comprehensive guide on how to do it, broken down into key steps:\\n\\n## What is RAG?\\n\\nRAG is a powerful technique that combines two core components:\\n\\n1.  **Retrieval:** This component searches a knowledge base (your documents) for relevant information to answer a user\\'s query.\\n2.  **Generation:** This component (typically a Large Language Model or LLM) takes the retrieved information and the user\\'s query to generate a coherent and informative answer.\\n\\nThis approach allows your chatbot to go beyond the general knowledge of an LLM and provide answers grounded in your specific data, reducing the risk of hallucinations and improving accuracy.\\n\\n## Steps to Build a Q&A Chatbot using RAG\\n\\nHere\\'s a step-by-step breakdown:\\n\\n### 1. Prepare Your Knowledge Base\\n\\nThis is the foundation of your RAG system.\\n\\n*   **Gather Your Documents:** Collect all the text-based documents you want your chatbot to learn from. This could include:\\n    *   Text files (.txt)\\n    *   PDFs (.pdf)\\n    *   Web pages (scraped content)\\n    *   Markdown files (.md)\\n    *   Word documents (.docx)\\n    *   Databases (requires extraction)\\n*   **Clean and Preprocess:**\\n    *   **Remove Irrelevant Content:** Get rid of headers, footers, page numbers, advertisements, or any other text that isn\\'t core to the information you want to retrieve.\\n    *   **Format Consistency:** Ensure a consistent format as much as possible.\\n    *   **Error Correction:** Fix typos and grammatical errors if they are prevalent.\\n*   **Chunk Your Documents:** LLMs have context window limitations. You can\\'t feed entire large documents at once. Therefore, you need to break them down into smaller, manageable \"chunks.\"\\n    *   **Chunking Strategies:**\\n        *   **Fixed-size chunks:** Divide text into fixed number of tokens or characters.\\n        *   **Sentence-based chunks:** Split text at sentence boundaries.\\n        *   **Paragraph-based chunks:** Split text at paragraph boundaries.\\n        *   **Semantic chunking:** More advanced, aims to keep related sentences together.\\n    *   **Overlap:** It\\'s often beneficial to have some overlap between chunks. This ensures that if a relevant piece of information spans across two chunks, the retrieval process can still find it.\\n\\n### 2. Create Embeddings\\n\\nEmbeddings are numerical representations (vectors) of your text chunks. These vectors capture the semantic meaning of the text, allowing you to compare them for similarity.\\n\\n*   **Choose an Embedding Model:** Several pre-trained embedding models are available. Popular choices include:\\n    *   **Sentence-Transformers:** A library offering various models (e.g., `all-MiniLM-L6-v2`, `multi-qa-mpnet-base-dot-v1`).\\n    *   **OpenAI Embeddings:** `text-embedding-ada-002` is a common choice.\\n    *   **Google\\'s Universal Sentence Encoder.**\\n*   **Generate Embeddings:** For each chunk of your preprocessed documents, use your chosen embedding model to generate a vector.\\n\\n### 3. Store Embeddings in a Vector Database\\n\\nA vector database is optimized for storing and querying high-dimensional vectors. It allows for efficient similarity searches.\\n\\n*   **Popular Vector Databases:**\\n    *   **ChromaDB:** Open-source, easy to set up locally.\\n    *   **FAISS (Facebook AI Similarity Search):** A library for efficient similarity search, often used with other storage solutions.\\n    *   **Pinecone:** A managed, cloud-based vector database.\\n    *   **Weaviate:** Open-source, GraphQL API, supports hybrid search.\\n    *   **Qdrant:** Open-source, high-performance vector similarity search engine.\\n*   **Indexing:** When you add your embeddings to the vector database, it creates an index to speed up search operations.\\n\\n### 4. Set Up Your Retrieval Mechanism\\n\\nThis is where the \"R\" in RAG comes into play. When a user asks a question:\\n\\n*   **Embed the User\\'s Query:** Use the *same* embedding model you used for your documents to generate an embedding for the user\\'s question.\\n*   **Perform a Similarity Search:** Query your vector database with the user\\'s query embedding. The database will return the \"k\" most similar document chunks (i.e., the chunks whose embeddings are closest to the query embedding in vector space). This \"k\" is a parameter you can tune.\\n\\n### 5. Set Up Your Generation Mechanism (LLM)\\n\\nThis is where the \"G\" in RAG comes into play.\\n\\n*   **Choose a Large Language Model (LLM):** Select an LLM that suits your needs. Options include:\\n    *   **OpenAI Models:** GPT-3.5 Turbo, GPT-4.\\n    *   **Google Models:** Gemini.\\n    *   **Open-Source Models:** Llama 2, Mistral, Falcon (can be hosted locally or on cloud providers).\\n*   **Prompt Engineering:** This is crucial for guiding the LLM to generate the desired output. Your prompt will typically include:\\n    *   **System Message/Instructions:** Define the role of the chatbot (e.g., \"You are a helpful AI assistant that answers questions based on the provided context.\").\\n    *   **Retrieved Context:** The relevant document chunks you retrieved in the previous step.\\n    *   **User\\'s Question:** The original query from the user.\\n    *   **Formatting Instructions:** How you want the answer to be presented.\\n\\n**Example Prompt Structure:**\\n\\n```\\nYou are a helpful AI assistant. Your task is to answer the user\\'s question based ONLY on the provided context. If you cannot find the answer in the context, state that you don\\'t have enough information.\\n\\nContext:\\n---\\n[Content of Chunk 1]\\n---\\n[Content of Chunk 2]\\n---\\n...\\n\\nUser Question:\\n[User\\'s actual question]\\n\\nAnswer:\\n```\\n\\n### 6. Integrate Retrieval and Generation\\n\\nThis is the core of your RAG chatbot.\\n\\n*   **User Input:** The user asks a question.\\n*   **Query Embedding:** Embed the user\\'s question.\\n*   **Vector Search:** Query the vector database to retrieve the most relevant document chunks.\\n*   **Prompt Construction:** Combine the retrieved chunks and the user\\'s question into a prompt for the LLM.\\n*   **LLM Generation:** Send the prompt to the LLM to generate an answer.\\n*   **Output to User:** Present the LLM\\'s generated answer to the user.\\n\\n### 7. Build the Chatbot Interface\\n\\nYou\\'ll need a way for users to interact with your RAG system.\\n\\n*   **Web Application:** Use frameworks like Flask, Django (Python), or React, Vue.js (JavaScript).\\n*   **Command-Line Interface (CLI):** For simpler implementations.\\n*   **Messaging Platforms:** Integrate with Slack, Discord, etc.\\n\\n### 8. Evaluation and Iteration\\n\\nBuilding a good RAG chatbot is an iterative process.\\n\\n*   **Test with Diverse Questions:** Cover a wide range of questions, including those that might be ambiguous or outside your knowledge base.\\n*   **Measure Accuracy:** How often does the chatbot provide correct answers?\\n*   **Assess Relevance:** Are the retrieved chunks truly relevant to the question?\\n*   **Evaluate Fluency and Coherence:** Is the generated answer easy to understand?\\n*   **Tune Parameters:** Experiment with chunk size, overlap, the number of retrieved documents (k), and LLM parameters.\\n*   **Improve Knowledge Base:** If the chatbot consistently fails on certain topics, consider adding more relevant documents or refining existing ones.\\n\\n## Tools and Libraries You Might Use\\n\\n*   **Python:** The most common language for this type of development.\\n*   **LangChain:** A popular framework that simplifies building LLM-powered applications, including RAG. It provides abstractions for document loading, text splitting, embedding, vector stores, LLMs, and prompt management.\\n*   **LlamaIndex (formerly GPT Index):** Another excellent framework specifically designed for connecting LLMs to external data. It excels at data ingestion, indexing, and querying.\\n*   **Hugging Face Transformers:** For accessing and using pre-trained embedding models and LLMs.\\n*   **Sentence-Transformers:** For easily generating sentence embeddings.\\n*   **PyPDF2, pdfminer.six:** For extracting text from PDFs.\\n*   **BeautifulSoup, Scrapy:** For web scraping.\\n*   **NumPy:** For numerical operations, especially with embeddings.\\n*   **OpenAI Python Client, Google Generative AI Python SDK:** For interacting with their respective LLM APIs.\\n\\n## Example Workflow with LangChain (Conceptual)\\n\\n```python\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings import OpenAIEmbeddings # Or HuggingFaceEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.llms import OpenAI # Or another LLM\\nfrom langchain.chains import RetrievalQA\\n\\n# 1. Load and Split Documents\\nloader = TextLoader(\"your_document.txt\")\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\n\\n# 2. Create Embeddings and Vector Store\\nembeddings = OpenAIEmbeddings()\\nvectorstore = Chroma.from_documents(texts, embeddings)\\n\\n# 3. Set up LLM\\nllm = OpenAI(temperature=0) # temperature=0 for more deterministic answers\\n\\n# 4. Create RetrievalQA Chain\\nqa_chain = RetrievalQA.from_chain_type(\\n    llm,\\n    retriever=vectorstore.as_retriever()\\n)\\n\\n# 5. Ask a Question\\nquery = \"What is the main topic of the document?\"\\nresponse = qa_chain({\"query\": query})\\n\\nprint(response[\"result\"])\\n```\\n\\n## Key Considerations and Challenges\\n\\n*   **Data Quality:** The quality of your input data directly impacts the chatbot\\'s performance.\\n*   **Chunking Strategy:** Experiment to find the optimal chunk size and overlap for your data.\\n*   **Embedding Model Choice:** Different models have varying strengths and weaknesses.\\n*   **LLM Choice and Cost:** Consider the performance, cost, and latency of the LLM you choose.\\n*   **Prompt Engineering:** This is an art and science. Fine-tuning your prompts can significantly improve results.\\n*   **Scalability:** For large knowledge bases and high traffic, you\\'ll need to consider the scalability of your vector database and LLM hosting.\\n*   **Hallucinations:** While RAG reduces hallucinations, they can still occur if the retrieved context is misleading or the LLM misinterprets it.\\n*   **Out-of-Context Questions:** Implement mechanisms to handle questions that cannot be answered from your knowledge base.\\n\\nBy following these steps and leveraging the right tools, you can build a powerful and accurate Q&A chatbot using the RAG approach. Good luck!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf9ed4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[{\"role\":\"system\", \"content\":\"You are a GenAI Developer\"},\n",
    "         {\"role\":\"user\", \"content\":\"Explain Langraph and Langchain in short.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeefa1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d90fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's break down LangGraph and LangChain in short:\\n\\n**LangChain:**\\n\\n*   **What it is:** A **framework** for building applications powered by large language models (LLMs).\\n*   **Think of it as:** A toolkit or a set of building blocks.\\n*   **Key purpose:** Simplifies the process of connecting LLMs with external data sources, other tools, and enabling complex LLM interactions.\\n*   **Core components:** Chains (sequences of LLM calls), Agents (LLMs that can use tools), Memory (for remembering past interactions), Document Loaders, Vector Stores, etc.\\n*   **Goal:** To make it easier to develop LLM-powered applications by providing abstractions and integrations.\\n\\n**LangGraph:**\\n\\n*   **What it is:** An **extension** of LangChain specifically designed for building **stateful, multi-actor applications** that can be represented as **graphs**.\\n*   **Think of it as:** A specialized tool for building more complex, dynamic, and iterative LLM workflows.\\n*   **Key purpose:** To manage and orchestrate complex sequences of LLM calls, tool usage, and decision-making that evolve over time. It excels at handling situations where the output of one step influences the input or logic of subsequent steps.\\n*   **Core concepts:** Nodes (individual steps or LLM calls), Edges (transitions between nodes), State (the information that flows between nodes), and a `Graph` object to define the execution flow.\\n*   **Goal:** To enable the creation of sophisticated LLM agents and workflows that can exhibit emergent behavior, collaborate, and adapt based on ongoing state.\\n\\n**In a nutshell:**\\n\\n*   **LangChain** is your **general-purpose LLM application builder**. It gives you the foundational pieces.\\n*   **LangGraph** is a **powerful add-on to LangChain** that specializes in building **complex, graph-based, and stateful LLM workflows**. It's for when your LLM application needs to have a more dynamic and evolving structure.\\n\\nYou can think of LangGraph as a more advanced way to structure and execute certain types of LangChain applications, particularly those involving agents that need to reason, plan, and iterate.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d74bc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2b7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8562f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ChatPromptTemplate.from_messages([{\"role\":\"system\", \"content\":\"You are a translator and translate into {language}\"},\n",
    "           {\"role\":\"user\", \"content\":\"{query}\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc7d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = prompts.format_messages(language=\"Hindi\", query=\"I like to play Cricket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5f4b365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a translator and translate into Hindi', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like to play Cricket', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58f103b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(final_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97aa5ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मुझे क्रिकेट खेलना पसंद है।'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a0fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मैं आर्टिफिशियल इंटेलिजेंस (एआई) सीखने में रुचि रखता हूँ।'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable\n",
    "# prompts.invoke({\"language\":\"Hindi\", \"query\": \"I am interested for learning Artificial Intelligence\"})\n",
    "\n",
    "# res = llm.invoke(prompts.invoke({\"language\":\"Hindi\", \"query\": \"I am interested for learning Artificial Intelligence\"})\n",
    "# )\n",
    "# res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b843cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = prompts | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490114ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a translator and translate into {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x000001F591A91D50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F591A91ED0>, root_client=<openai.OpenAI object at 0x000001F5919D3F70>, root_async_client=<openai.AsyncOpenAI object at 0x000001F591A92110>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chains.invoke({\"language\":\"Hindi\", \"query\": \"I am interested for learning Artificial Intelligence\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f49930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc50c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "out = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3bd15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## My self chain node \n",
    "def transform_case(res: str):\n",
    "    return res.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56df0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ChatPromptTemplate.from_messages([{\"role\":\"system\", \"content\":\"You are a translator and translate into {language}\"},\n",
    "           {\"role\":\"user\", \"content\":\"{query}\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bbc54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = prompts | llm | out | transform_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fb5e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chains.invoke({\"language\":\"Hindi\", \"query\": \"I am interested for learning Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e048d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मैं आर्टिफिशियल इंटेलिजेंस सीखने में रुचि रखता हूँ।'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
