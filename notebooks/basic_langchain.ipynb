{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e35a617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f486e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a4cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(\"Who was the winner of the IPL 2016?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544144cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The winner of the Indian Premier League (IPL) 2016 was Sunrisers Hyderabad. They won the title by defeating Royal Challengers Bangalore in the final match.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "818e429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a63bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is GenAI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ada553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GenAI stands for **Generative Artificial Intelligence**.\\n\\nIn essence, GenAI refers to a type of artificial intelligence that can **create new content**, rather than just analyze or act upon existing data. This content can take many forms, including:\\n\\n*   **Text:** Writing articles, stories, poems, code, emails, summaries, and even engaging in conversations.\\n*   **Images:** Generating realistic or artistic images from text prompts, editing existing images, or creating entirely new visual styles.\\n*   **Audio:** Composing music, generating speech, or creating sound effects.\\n*   **Video:** Producing short video clips, animating characters, or synthesizing realistic human performances.\\n*   **Code:** Writing software code in various programming languages.\\n*   **3D Models:** Creating three-dimensional objects and environments.\\n\\n**How does it work?**\\n\\nGenAI models are typically trained on massive datasets of existing content. Through this training, they learn patterns, structures, relationships, and styles within that data. When given a prompt or instruction, they use this learned knowledge to generate novel output that is similar in style and content to what they were trained on, but is entirely new.\\n\\n**Key characteristics of GenAI:**\\n\\n*   **Creativity:** Its primary distinguishing feature is its ability to generate novel and original content.\\n*   **Learning from Data:** It relies on vast amounts of data to learn patterns and generate new outputs.\\n*   **Prompt-Driven:** Users often interact with GenAI models by providing text prompts or other inputs to guide the generation process.\\n*   **Versatility:** It can be applied to a wide range of creative tasks.\\n\\n**Examples of popular GenAI models and applications:**\\n\\n*   **Large Language Models (LLMs):** ChatGPT (OpenAI), Bard/Gemini (Google), Claude (Anthropic) are prominent examples that excel at text generation and understanding.\\n*   **Image Generators:** DALL-E (OpenAI), Midjourney, Stable Diffusion are well-known for their ability to create images from text descriptions.\\n*   **Code Generators:** GitHub Copilot (Microsoft/GitHub) assists developers by suggesting and generating code.\\n\\n**Implications and Applications:**\\n\\nGenAI has the potential to revolutionize many industries and aspects of our lives, including:\\n\\n*   **Content Creation:** Accelerating writing, art, music, and video production.\\n*   **Software Development:** Automating coding tasks and improving developer productivity.\\n*   **Education:** Creating personalized learning materials and interactive tutors.\\n*   **Design:** Generating design concepts and prototypes.\\n*   **Research:** Assisting with hypothesis generation and data analysis.\\n*   **Entertainment:** Creating new forms of interactive stories and games.\\n\\n**Challenges and Considerations:**\\n\\nWhile exciting, GenAI also presents challenges and ethical considerations, such as:\\n\\n*   **Accuracy and Bias:** GenAI models can sometimes generate inaccurate or biased information due to the data they are trained on.\\n*   **Misinformation and Deepfakes:** The ability to create realistic fake content raises concerns about misinformation and malicious use.\\n*   **Copyright and Ownership:** Questions surrounding the ownership and copyright of AI-generated content are still being debated.\\n*   **Job Displacement:** The automation of certain creative tasks could lead to job displacement in some sectors.\\n*   **Environmental Impact:** Training large GenAI models requires significant computational resources and energy.\\n\\nIn summary, GenAI is a transformative technology that empowers machines to create new content, opening up a world of possibilities and also presenting important questions that society needs to address.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a7d8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[(\"system\", \"You are a GenAI Developer\"),\n",
    "         (\"user\", \"Tell me how to make a Q-NA Chatbot using RAG\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e65798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2e8067a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let\\'s break down how to build a Q&A Chatbot using Retrieval-Augmented Generation (RAG) in R.\\n\\n**What is RAG?**\\n\\nRAG is a powerful technique that combines the strengths of two key components:\\n\\n1.  **Retrieval:** This part involves searching a knowledge base (your documents, articles, FAQs, etc.) for relevant information based on a user\\'s query.\\n2.  **Generation:** This part uses a large language model (LLM) to synthesize the retrieved information into a coherent and conversational answer.\\n\\n**Why RAG for Q&A Chatbots?**\\n\\n*   **Accuracy and Factuality:** By grounding the LLM\\'s responses in specific documents, RAG reduces the chances of the LLM \"hallucinating\" or making up information.\\n*   **Up-to-date Information:** You can easily update your knowledge base without retraining the LLM.\\n*   **Domain-Specific Knowledge:** You can tailor the chatbot to a specific domain by providing relevant documents.\\n*   **Transparency:** You can often point to the source documents that informed the answer.\\n\\n**Steps to Build a RAG Q&A Chatbot in R**\\n\\nHere\\'s a conceptual outline and then we\\'ll dive into R-specific implementation details.\\n\\n**Phase 1: Data Preparation and Indexing**\\n\\n1.  **Gather Your Knowledge Base:** Collect all the documents (PDFs, TXTs, Markdown, web pages, etc.) that contain the information your chatbot should draw from.\\n2.  **Chunking:** Large documents need to be split into smaller, manageable \"chunks.\" This is crucial for effective retrieval. Each chunk should ideally contain a coherent piece of information.\\n3.  **Embedding:** Convert each text chunk into a numerical vector representation (an embedding) using an embedding model. Similar text chunks will have similar embeddings.\\n4.  **Vector Database/Index:** Store these embeddings and their corresponding text chunks in a searchable index. This allows for efficient similarity search.\\n\\n**Phase 2: Retrieval and Generation**\\n\\n1.  **User Query:** The user asks a question.\\n2.  **Query Embedding:** Convert the user\\'s query into an embedding using the same embedding model used for your documents.\\n3.  **Similarity Search:** Use the query embedding to search the vector database for the most similar document chunk embeddings. Retrieve the text content of these top-k relevant chunks.\\n4.  **Prompt Engineering:** Construct a prompt for the LLM. This prompt will include:\\n    *   Instructions for the LLM (e.g., \"Answer the following question based on the provided context.\").\\n    *   The retrieved text chunks (the \"context\").\\n    *   The user\\'s original question.\\n5.  **LLM Generation:** Send the engineered prompt to an LLM. The LLM will use the provided context and its own general knowledge to generate an answer.\\n6.  **Output:** Present the LLM\\'s generated answer to the user.\\n\\n**R Implementation - Key Libraries and Concepts**\\n\\nR has a growing ecosystem for AI and NLP. Here are the core libraries you\\'ll likely use:\\n\\n*   **`reticulate`:** This is essential for bridging R with Python. Many of the most powerful NLP and LLM libraries are in Python. You\\'ll use `reticulate` to call Python functions and use Python objects from within R.\\n*   **Text Processing Libraries (R):**\\n    *   `stringr` or base R string functions for basic text manipulation.\\n    *   `tm` or `quanteda` for more advanced text mining tasks like tokenization, stop-word removal, etc. (though for RAG, simpler chunking might be sufficient).\\n*   **Embedding Models and Vector Databases (via Python):**\\n    *   **`sentence-transformers` (Python):** A fantastic Python library for generating high-quality sentence and text embeddings. You\\'ll access this via `reticulate`.\\n    *   **`Faiss` (Python):** A library for efficient similarity search and clustering of dense vectors. Excellent for building your vector index.\\n    *   **`ChromaDB` or `Pinecone` (Python/Cloud):** Managed vector databases that offer more features and scalability if you\\'re building a production system.\\n*   **Large Language Models (LLMs) (via Python):**\\n    *   **`transformers` (Python):** The Hugging Face `transformers` library is the de facto standard for working with pre-trained LLMs. You\\'ll use this to load and run models.\\n    *   **`openai` (Python):** If you\\'re using OpenAI\\'s API (GPT-3.5, GPT-4), you\\'ll use their Python client.\\n    *   **`langchain` (Python):** A very popular framework for building LLM applications, including RAG. It simplifies many of the steps involved. You can and should use `langchain` via `reticulate`.\\n\\n**Let\\'s Get Practical (Conceptual R Code)**\\n\\nSince R doesn\\'t have as mature native libraries for all these components as Python, we\\'ll heavily rely on `reticulate` to call Python.\\n\\n**Step 1: Setup and Environment**\\n\\nFirst, ensure you have Python installed and that R can find it. You\\'ll also need to install the necessary Python packages.\\n\\n```R\\n# In your R console:\\n# Install reticulate if you haven\\'t already\\n# install.packages(\"reticulate\")\\n\\n# Configure reticulate to use your Python environment (e.g., a virtual environment)\\n# reticulate::use_python(\"/path/to/your/python/executable\")\\n# or\\n# reticulate::use_virtualenv(\"your_env_name\")\\n\\n# Install Python packages (run this in your R console, or in your Python environment directly)\\n# reticulate::py_install(\"sentence-transformers\")\\n# reticulate::py_install(\"faiss-cpu\") # or faiss-gpu if you have a GPU\\n# reticulate::py_install(\"transformers\")\\n# reticulate::py_install(\"torch\") # or tensorflow if you prefer\\n# reticulate::py_install(\"langchain\")\\n# reticulate::py_install(\"openai\") # if you plan to use OpenAI API\\n```\\n\\n**Step 2: Data Preparation and Indexing (Conceptual)**\\n\\n```R\\nlibrary(reticulate)\\n\\n# --- Python Code (executed via reticulate) ---\\n\\n# Import necessary Python libraries\\npy_run_string(\"\\nfrom sentence_transformers import SentenceTransformer\\nimport faiss\\nimport numpy as np\\nimport os # For potentially loading files\\n\\n# Function to load and chunk documents (simplified)\\ndef load_and_chunk_documents(file_paths, chunk_size=200, overlap=50):\\n    documents = []\\n    for file_path in file_paths:\\n        with open(file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            text = f.read()\\n            # Simple chunking logic\\n            for i in range(0, len(text), chunk_size - overlap):\\n                chunk = text[i:i + chunk_size]\\n                if chunk.strip(): # Avoid empty chunks\\n                    documents.append({\\'text\\': chunk, \\'source\\': os.path.basename(file_path)})\\n    return documents\\n\\n# Function to create embeddings and FAISS index\\ndef create_rag_index(documents, model_name=\\'all-MiniLM-L6-v2\\'):\\n    model = SentenceTransformer(model_name)\\n    texts = [doc[\\'text\\'] for doc in documents]\\n    embeddings = model.encode(texts, convert_to_numpy=True)\\n\\n    # Create FAISS index\\n    dimension = embeddings.shape[1]\\n    index = faiss.IndexFlatL2(dimension) # L2 distance for similarity\\n    index.add(np.array(embeddings))\\n\\n    return model, index, documents\\n\\n# --- R Code to call Python ---\\n\\n# Define your document paths\\ndocument_files <- c(\"path/to/your/doc1.txt\", \"path/to/your/doc2.md\") # Replace with actual paths\\n\\n# Execute Python code to create the index\\npy$rag_data <- py_capture_output({\\n  documents_py <- load_and_chunk_documents(document_files)\\n  model_py, index_py, documents_with_meta_py <- create_rag_index(documents_py)\\n  list(model=model_py, index=index_py, documents=documents_with_meta_py)\\n})\\n\\n# You can now access these from R (though the index object might be complex to use directly)\\n# For simplicity, we\\'ll re-encode query in R and pass it to Python\\nembedding_model <- py$rag_data$model\\ndocument_chunks <- py$rag_data$documents # This is a list of dicts in Python, R might represent it differently\\n# A better approach is to save the FAISS index to disk if it\\'s large.\\n```\\n\\n**Step 3: Retrieval and Generation (Conceptual)**\\n\\n```R\\nlibrary(reticulate)\\n\\n# Assume py$embedding_model, py$index, and py$document_chunks are available from Step 2\\n\\n# Function to perform retrieval and generation\\ndef answer_question_with_rag(question, model, index, documents, k=3):\\n    # Encode the question\\n    question_embedding = model.encode([question], convert_to_numpy=True)\\n\\n    # Perform similarity search using FAISS\\n    distances, indices = index.search(question_embedding, k) # k is the number of nearest neighbors\\n\\n    # Get the most relevant document chunks\\n    retrieved_contexts = []\\n    for i in indices[0]:\\n        retrieved_contexts.append(documents[i][\\'text\\'])\\n\\n    # Construct the prompt for the LLM\\n    context_str = \"\\\\\\\\n\".join(retrieved_contexts)\\n    prompt = f\\\\\"\\\\\"\\\\\"Based on the following context, answer the question.\\n    Context:\\n    {context_str}\\n\\n    Question: {question}\\n\\n    Answer:\\\\\"\\\\\"\\\\\"\\n\\n    # --- LLM Generation (using a placeholder or actual LLM call) ---\\n    # This part is highly dependent on your LLM choice.\\n    # Example using a hypothetical LLM function\\n    # generated_answer = call_my_llm_api(prompt)\\n    # For demonstration, let\\'s just return the prompt and context\\n    return prompt, retrieved_contexts\\n\\n# --- R Code to call Python ---\\n\\nuser_query <- \"What is the main purpose of the document?\"\\n\\n# Execute Python code for retrieval and generation\\npy$rag_response <- py_capture_output({\\n  answer_question_with_rag(user_query, py$rag_data$model, py$rag_data$index, py$rag_data$documents)\\n})\\n\\n# Extract and display the results\\ngenerated_prompt <- py$rag_response[[1]]\\nretrieved_docs <- py$rag_response[[2]]\\n\\ncat(\"--- Retrieved Documents ---\\\\n\")\\nfor (doc in retrieved_docs) {\\n  cat(paste0(\"- \", doc, \"\\\\n\"))\\n}\\n\\ncat(\"\\\\n--- Generated Prompt ---\\\\n\")\\ncat(generated_prompt)\\n\\n# To get the actual LLM answer, you would integrate an LLM API call here.\\n# For example, using OpenAI:\\n# py_run_string(\"\\n# import openai\\n# openai.api_key = \\'YOUR_API_KEY\\'\\n#\\n# def call_openai_llm(prompt_text):\\n#     response = openai.ChatCompletion.create(\\n#         model=\\'gpt-3.5-turbo\\', # Or \\'gpt-4\\'\\n#         messages=[\\n#             {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant.\\'},\\n#             {\\'role\\': \\'user\\', \\'content\\': prompt_text}\\n#         ]\\n#     )\\n#     return response.choices[0].message.content\\n# \")\\n#\\n# final_answer <- py$call_openai_llm(generated_prompt)\\n# cat(\"\\\\n--- Final Answer ---\\\\n\")\\n# cat(final_answer)\\n\\n```\\n\\n**Using `langchain` (Recommended Approach)**\\n\\n`langchain` significantly simplifies RAG implementation. You can use its Python library via `reticulate`.\\n\\n**Step 1: Install `langchain` and its dependencies in Python.**\\n\\n```R\\n# In R console:\\nreticulate::py_install(\"langchain\")\\nreticulate::py_install(\"langchain-community\") # For many integrations\\nreticulate::py_install(\"openai\") # If using OpenAI\\nreticulate::py_install(\"sentence-transformers\")\\nreticulate::py_install(\"faiss-cpu\")\\n```\\n\\n**Step 2: Conceptual R code using `langchain`**\\n\\n```R\\nlibrary(reticulate)\\n\\n# --- Python Code (executed via reticulate) ---\\npy_run_string(\"\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.embeddings import SentenceTransformerEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain.chains import RetrievalQA\\nfrom langchain_openai import ChatOpenAI # Or other LLM providers\\n\\n# --- Configuration ---\\n# Replace with your actual document paths\\ndocument_paths = [\\'path/to/your/doc1.txt\\', \\'path/to/your/doc2.md\\']\\nembedding_model_name = \\'all-MiniLM-L6-v2\\'\\nllm_model_name = \\'gpt-3.5-turbo\\' # Or \\'gpt-4\\'\\nopenai_api_key = \\'YOUR_OPENAI_API_KEY\\' # IMPORTANT: Set your API key securely\\n\\n# --- 1. Load Documents ---\\nall_docs = []\\nfor path in document_paths:\\n    loader = TextLoader(path, encoding=\\'utf-8\\')\\n    all_docs.extend(loader.load())\\n\\n# --- 2. Split Documents ---\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nsplits = text_splitter.split_documents(all_docs)\\n\\n# --- 3. Create Embeddings and Vector Store ---\\n# Initialize embedding model\\nembeddings = SentenceTransformerEmbeddings(model_name=embedding_model_name)\\n\\n# Create FAISS vector store from document splits\\n# Langchain handles the FAISS index creation and storage internally\\nvectorstore = FAISS.from_documents(splits, embeddings)\\n\\n# --- 4. Initialize LLM ---\\nllm = ChatOpenAI(model_name=llm_model_name, openai_api_key=openai_api_key, temperature=0.7)\\n\\n# --- 5. Create RetrievalQA Chain ---\\n# This chain orchestrates retrieval and generation\\nqa_chain = RetrievalQA.from_chain_type(\\n    llm=llm,\\n    chain_type=\\'stuff\\', # \\'stuff\\' puts all retrieved docs into one prompt\\n    retriever=vectorstore.as_retriever(search_kwargs={\\'k\\': 3}) # Retrieve top 3 documents\\n)\\n\\n# --- Function to query the chain ---\\ndef query_rag_chain(question):\\n    result = qa_chain.invoke({\\'query\\': question})\\n    return result[\\'result\\']\\n\\n\")\\n\\n# --- R Code to interact with the Langchain setup ---\\n\\n# Set your OpenAI API key if you haven\\'t already in your environment\\n# Sys.setenv(OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\")\\n\\nuser_question <- \"What are the key benefits mentioned in the documents?\"\\n\\n# Query the RAG chain\\nanswer <- py$query_rag_chain(user_question)\\n\\ncat(\"User Question:\", user_question, \"\\\\n\")\\ncat(\"Chatbot Answer:\", answer, \"\\\\n\")\\n\\n# You can also directly access the retriever and LLM if you need more control\\n# py$vectorstore\\n# py$llm\\n```\\n\\n**Key Considerations and Next Steps**\\n\\n*   **Chunking Strategy:** Experiment with `chunk_size` and `overlap`. Too small chunks might lose context; too large might exceed LLM token limits or dilute relevance.\\n*   **Embedding Model Choice:** `all-MiniLM-L6-v2` is a good starting point. For more specialized domains, consider larger or fine-tuned models.\\n*   **Vector Database:** For production, consider managed solutions like Pinecone, Weaviate, or cloud-based FAISS deployments for scalability and features.\\n*   **LLM Choice:** OpenAI\\'s GPT models are powerful. For cost-effectiveness or specific needs, explore Hugging Face models (e.g., Llama, Mistral) via the `transformers` library.\\n*   **Prompt Engineering:** The prompt you send to the LLM is critical. Experiment with different phrasing and instructions.\\n*   **Evaluation:** How do you know if your chatbot is good? Develop metrics to evaluate accuracy, relevance, and fluency.\\n*   **User Interface:** For a real chatbot, you\\'ll need a way for users to interact. This could be a web app (e.g., using `shiny` in R, or Flask/Streamlit in Python) or a command-line interface.\\n*   **Error Handling and Robustness:** Implement checks for file loading, API errors, etc.\\n*   **Security:** Be mindful of API keys and sensitive data.\\n\\nThis comprehensive guide should give you a solid foundation for building your RAG Q&A chatbot in R, leveraging the power of Python\\'s NLP ecosystem through `reticulate`. Remember that RAG is an iterative process, so be prepared to experiment and refine your approach.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf9ed4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[{\"role\":\"system\", \"content\":\"You are a GenAI Developer\"},\n",
    "         {\"role\":\"user\", \"content\":\"Explain Langraph and Langchain in short.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeefa1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08d90fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a short explanation of LangGraph and LangChain:\\n\\n**LangChain:**\\n\\nLangChain is a **framework for developing applications powered by large language models (LLMs)**. It provides tools and abstractions to chain together LLMs with other components like data sources, APIs, and memory, enabling you to build more complex and capable LLM applications. Think of it as a toolkit for orchestrating LLMs and their interactions.\\n\\n**LangGraph:**\\n\\nLangGraph is an **extension of LangChain specifically designed for building stateful, multi-agent applications that involve loops or cycles**. It allows you to define a graph where nodes represent steps in your application (which can be LLM calls, tool usage, etc.) and edges define the flow between these steps. The key innovation is its ability to handle **conditional branching and looping**, making it ideal for scenarios where an application needs to repeatedly perform actions based on intermediate results, like in a complex planning or reasoning process.\\n\\n**In essence:**\\n\\n*   **LangChain** is about **connecting LLMs to other things** to build applications.\\n*   **LangGraph** is about **building applications with LLMs that can loop and reason over time**, by defining a graph of states and transitions.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bc0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
